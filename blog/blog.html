<!DOCTYPE html>
<html>

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-115627522-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-115627522-1');

    </script>


    <title>Blog</title>
    <link href="https://fonts.googleapis.com/css?family=Lato: 100,300,400,700|Luckiest+Guy|Oxygen:300,400" rel="stylesheet">

    <!-- Mobile specific meta
    --------------------------------------------------------- -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- LaTeX
    ---------------------------------------------------------- -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async src="https://example.com/mathjax/MathJax.js?config=TeX-AMS_CHTML">
    </script>

    <!-- CSS 
    --------------------------------------------------------- -->
    <link rel="stylesheet" href="../css/normalize.css">
    <link rel="stylesheet" href="../css/skeleton.css">
    <link rel="stylesheet" href="../css/custom.css">
    <link href="../css/style.css" type="text/css" rel="stylesheet">

    <meta http-equiv="content-type" content="text/html;charset=utf-8" />


</head>

<body>
    <!-- NavBar 
    --------------------------------------------------------- -->
    <div class="navbar-spacer"></div>
    <nav class="navbar">
        <div class="container">
            <ul class="navbar-list">
                <li class="navbar-item"><a class="navbar-link" href="../index.html">Home</a></li>
                <li class="navbar-item"><a class="navbar-link" href="../code/code.html">Code</a></li>
                <li class="navbar-item"><a class="navbar-link" href="../resources/resources.html">Resources</a></li>
                <li class="navbar-item"><a class="navbar-link" href="./blog.html">Blog</a></li>

            </ul>
        </div>
    </nav>

    <div class="container">
        <div class="row">
            <div class="eight columns">
                <h3 id="intuitionOnLinearRegression">Intuition on Linear Regression</h3>
                <h6>December 2018</h6>
                <hr>
                <p>
                    In the linear regression we aim to explain variablity in our dependent variable "Y" by variability in our independent variables "X" ($x_1, x_2, ...$). We would like to separate possible explanation through our independent variables from the rest of influencing factors (variables in the u (error) term). In the following example, we first construct our model without the intercept. Second, we only keep the intercept, and finally we keep both intercept and our independent variable. These are the most basic models that help us to be able to imagine how regression works in action. <br><br>
                    The simplest form of our example is a regression model with an independent variable and only two observations. <br><br>
                    Our two observations are:<br>
                    $$
                    \bigg[
                    \begin{matrix}
                    X_1\\
                    X_2
                    \end{matrix}
                    \bigg]
                    =
                    \bigg[
                    \begin{matrix}
                    3\\
                    1
                    \end{matrix}
                    \bigg]

                    and

                    \bigg[
                    \begin{matrix}
                    Y_1\\
                    Y_2
                    \end{matrix}
                    \bigg]
                    =
                    \bigg[
                    \begin{matrix}
                    -1\\
                    \phantom{-}1
                    \end{matrix}
                    \bigg]
                    $$
                    \begin{multline*}
                    \end{multline*}
                    Considering the following specification:<br>
                    $$
                    \bigg[
                    \begin{matrix}
                    Y_1\\
                    Y_2
                    \end{matrix}
                    \bigg]
                    =
                    \bigg[
                    \begin{matrix}
                    X_1\\
                    X_2
                    \end{matrix}
                    \bigg]
                    \beta_1
                    +
                    \bigg[
                    \begin{matrix}
                    u_1\\
                    u_2
                    \end{matrix}
                    \bigg]
                    $$
                    \begin{multline*}
                    \end{multline*}
                    To find the value for $\beta_1$, we first need to $\min_{\beta_1} \sum_i (y_i-\beta_1x_i)^2$, which means: $\min_{\beta_1}(-1-3\beta_1)^2 +(1-\beta_1)^2$, to do this we take the first derivative:

                    $$
                    \begin{align*}
                    (2*(-1-3 \hat{\beta}_1 )*-3) +(2*(1- \hat{\beta}_1 )*-1) = 0
                    \\
                    \rightarrow 6+18 \hat{\beta}_1 -2+2 \hat{\beta}_1 =0
                    \\
                    \rightarrow 20 \hat{\beta}_1 =-4
                    \\
                    \rightarrow \thinspace { \hat{\beta}_1 = -\frac{4}{20}=-\frac{1}{5}}
                    \end{align*}
                    $$


                    \begin{multline*}
                    \end{multline*}

                    In the observation space, Y and X create vectors. Since in the example we have two observations that create a two-dimensional space including the vectors as shown in the following graph:
                    <img src="../images/beta_1-.png" width="600" class="imgblog" />
                    Since in our first model we have not included the intercept, $\beta_1X$ create the line along the X vector. In the OLS approach, the projection of vector Y on the line $\beta_1X$ is a new vector $\hat{\beta}_1X$ (estimated $\beta_1$) which is the best thing we can do in explaining variablity in Y using our observations.
                    <br><br>
                    Now, imagine we only have a model with the intercept. Considering the following specification:
                    $$
                    \bigg[
                    \begin{matrix}
                    Y_1\\
                    Y_2
                    \end{matrix}
                    \bigg]
                    =
                    \bigg[
                    \begin{matrix}
                    1\\
                    1
                    \end{matrix}
                    \bigg]
                    \beta_0
                    +
                    \bigg[
                    \begin{matrix}
                    u_1\\
                    u_2
                    \end{matrix}
                    \bigg]
                    $$
                    \begin{multline*}
                    \end{multline*}
                    To find the value for $\beta_0$, we first need to $\min_{\beta_0} \sum_i (y_i-\beta_0)^2$, which means: $\min_{\beta_0}(-1-\beta_0)^2 +(1-\beta_0)^2$, to do this we take the first derivative:
                    \begin{align*}
                    (2*(-1- \hat{\beta}_0)*-1) +(2*(1- \hat{\beta}_0)*-1)=0
                    \\
                    \rightarrow 2+2 \hat{\beta}_0-2+2\hat{\beta}_0=0
                    \\
                    \rightarrow \thinspace {\hat{\beta}_0=0}
                    \end{align*}
                    \begin{multline*}
                    \end{multline*}
                    Heving only intercept in the model means in the observation space, $\beta_0\bigg[
                    \begin{matrix}
                    1\\
                    1
                    \end{matrix}
                    \bigg]$ creates the line on the unit vector $\bigg[
                    \begin{matrix}
                    1\\
                    1
                    \end{matrix}
                    \bigg]$
                    In this special case, vector Y is orthogonal to the unit vector and $\beta_0\bigg[
                    \begin{matrix}
                    1\\
                    1
                    \end{matrix}
                    \bigg]$. As a result, $\hat{\beta}_0 \bigg[
                    \begin{matrix}
                    1\\
                    1
                    \end{matrix}
                    \bigg]=\bigg[
                    \begin{matrix}
                    0\\
                    0
                    \end{matrix}
                    \bigg]$ that means $\hat{\beta}_0=0$.
                    <img src="../images/beta_0-.png" width="600" class="imgblog" />

                    Before including both the intercept and independent variable in our model, it is important to note how the model specifications in our two model impact the sum of residuals. In the first model:
                    $$
                    \begin{align*}
                    \hat{y}_1=-\frac{1}{5}*3=-\frac{3}{5} \rightarrow \hat{u}_1=Y_1-\hat{y}_1=-1-(-\frac{3}{5})=-\frac{2}{5}\\
                    \hat{y}_2=-\frac{1}{5}*1=-\frac{1}{5} \rightarrow \hat{u}_2=Y_2-\hat{y}_2=1-(-\frac{1}{5})=\frac{6}{5}\\
                    \longrightarrow \sum \hat{u}_i = -\frac{2}{5} + \frac{6}{5} = \frac{4}{5}
                    \end{align*}
                    $$
                    Since in the first model we only estimate $\beta_1$, $\beta_0$ is forced to be equal to zero. When we do not include $\beta_0$ in our model, the first algebraic property of OLS approach does not hold; meaning, the sum of residuals does not add up to zero by construction.In the second model:
                    $$
                    \begin{align*}
                    \hat{y}_1=0 \rightarrow \thinspace \hat{u}_1=Y_1-\hat{y}_1=-1-0=-1\\
                    \hat{y}_2=0 \rightarrow \thinspace \hat{u}_2=Y_2-\hat{y}_2=1-0=1\\
                    \longrightarrow \sum \hat{u}_i = -1 + 1 = 0
                    \end{align*}
                    $$
                    Because of the first algebraic property of OLS the sum of residuals adds up to zero.
                    <br><br>
                    in the two dimensional space created by our two observations, having both $\beta_0$ and $\beta_1$ create a plane and the Y vector is already on that plane. This means by including both $\beta_0$ and $\beta_1$ we can explain all the variability in Y. To find the value for $\beta_0$ and $\beta_1$, we first need to $\min_{\beta_0,\beta_1} \sum_i (y_i-(\beta_0+\beta_1x_i))^2$, which means:
                    $$

                    \min_{\beta_0,\beta_1}(-1-(\beta_0+(3\beta_1)))^2
                    +(1-(\beta_0+\beta_1))^2$

                    $$
                    to do this we take the first derivative in respect to each parameter: with respect to $\beta_0$:
                    $$
                    \begin{align*}
                    (2 * (-1-(\beta_0+(3\beta_1))) * -1) + (2 * (1-(\beta_0+\beta_1)) * -1) = 0 \\
                    \rightarrow 2+2\beta_0+6\beta_1 -2 +2\beta_0+2\beta_1=4\beta_0+8\beta_1=0 \\
                    \rightarrow \beta_0 = -2\beta_1
                    \end{align*}
                    $$
                    with respect to $\beta_1$:
                    $$
                    \begin{align*}
                    (2 * (-1-( \beta_0 +(3\beta_1))) * -3) + (2 * (1-( \beta_0 + \beta_1) ) * -1) = 0 \\
                    \rightarrow 6+6 \beta_0 +18 \beta_1 -2 +2 \beta_0 +2 \beta_1 = 4+8 \beta_0+20 \beta_1=0 \\
                    \rightarrow 1+2 \beta_0+5 \beta_1 = 0 \rightarrow 1-4\beta_1+5\beta_1=0 \\ \rightarrow \beta_1 = -1 \& \beta_0 = 2
                    \end{align*}
                    $$

                    <img src="../images/beta_0-and-beta_1-.png" width="600" class="imgblog" />

                    Explaining all the variablity means our estimated line passes through all the observation pairs - perfectly fitted.

                    <img src="../images/line.png" width="600" class="imgblog" />

                    Remember that in this example we only had two observations. If we have three observations we would have a three-dimensional space in which we try to project vector Y on the plane created by $\beta_0$ and $\beta_1$. This would not change the logic we use to address this kind of problems. <br> <br> When I learned about the graphical intuition of linear regression many more issues started to make sense. This is thanks to Angelo Secchi who introduced us to this way of thinking about line fitting problems.
                </p>
            </div>

        </div>
    </div>
    <div id="sidebar">

        <ul>
            <li><a href="#">■ Intuition on Linear Regression</a></li><br>
            <!--      <li><a href="#">About Me</a></li><br>
          <li><a href="#">Recent Posts</a></li><br>
          <li><a href="#">Contact</a></li>     -->
        </ul>
    </div>
<div class="footer">
        <div class="row">
            <div class="four columns">
                <h6 class="footer_h6">Contact</h6>
                <p class="footer_h6">
                    &#x02609 parham.ashur[at]etu-univ1[dot]fr
                    <br>
                    &#x02609 29 rue d'Ulm, Paris</p>
            </div>
            <div class="three columns">
                <h6 class="footer_h6">Code</h6>
                <p class="footer_h6">
                    <a href="code/code.html#r" id="linkcolor_footer">&#10070 R</a><br>
                   <!--  <a href="code/code.html#c" id="linkcolor_footer">&#10070 C for Arduino</a><br> -->
                    <a href="code/code.html#shell" id="linkcolor_footer"> &#10070 Shell Script</a><br>
                    <a href="code/code.html#python" id="linkcolor_footer">&#10070 Python</a><br>
                    <a href="code/code.html#stata" id="linkcolor_footer">&#10070 Stata</a><br></p>
            </div>
            <div class="three columns">
                <h6 class="footer_h6">Resources</h6>
                <p class="footer_h6">
                    <a href="https://learnpythonthehardway.org/book/" target="_blank" id="linkcolor_footer">&#10070 Learn Python</a><br>
                    <a href="https://www.latex-tutorial.com" target="_blank" id="linkcolor_footer">&#10070 Learn Latex</a><br>
                    <a href="https://swirlstats.com" target="_blank" id="linkcolor_footer">&#10070 Learn R</a><br>
                </p>
            </div>
            <div class="two columns">
                <h6 class="footer_h6">Links</h6>
                <p class="footer_h6">
                    <a href="https://www.upwork.com/o/profiles/users/_~016515787f8dbe61ea/" target="_blank" id="linkcolor_footer">&#10070 Upwork</a><br>

                    <a href="https://github.com/parhamashur" target="_blank" id="linkcolor_footer">&#10070 Github</a><br>
                    <!--        <a href="https://www.latex-tutorial.com" target="_blank" id="linkcolor_footer">&#10070 Learn Latex</a><br>
                    <a href="https://swirlstats.com" target="_blank" id="linkcolor_footer">&#10070 Learn R</a><br> -->
                </p>
            </div>
        </div>
    </div>

</body></html>
